{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65TOTJBV1pdX",
        "outputId": "2fd63f97-6e7f-443c-ba09-67907b4e701f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MNiQZeDh4Xkl",
        "outputId": "f5d563f6-0e7b-42b6-be26-d63def7a0176"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install fairseq\n",
        "!pip install fastBPE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.23)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.9.0+cu102)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.2.0)\n",
            "Collecting omegaconf==2.1.*\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=01ea75570427ac0c0c6a7e62e8f17dae07ffe5e89fd7de76b9f2baa64fa35b20\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, sacrebleu, hydra-core, dataclasses, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.0 omegaconf-2.1.0 portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "Building wheels for collected packages: fastBPE\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl size=483106 sha256=fa8c20ab72f4015ee93bfc8adf53194c35b9a8a4ff713053b2001629e5845fc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d4/0e/0d317a65f77d3f8049fedd8a2ee0519164cf3e6bd77ef886f1\n",
            "Successfully built fastBPE\n",
            "Installing collected packages: fastBPE\n",
            "Successfully installed fastBPE-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynmdz9ZHzjZ7",
        "outputId": "11bfcd66-d2cc-4967-ed81-2e93b920e3f6"
      },
      "source": [
        "!git clone https://github.com/vncorenlp/VnCoreNLP\n",
        "!pip install vncorenlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VnCoreNLP'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 215 (delta 0), reused 0 (delta 0), pack-reused 212\u001b[K\n",
            "Receiving objects: 100% (215/215), 214.22 MiB | 33.41 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=44d0538af5546e80f5568a10232d0663bd133e1282f964967e6fea0894d32204\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jiA5nJZzZ1J"
      },
      "source": [
        "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "from vncorenlp import VnCoreNLP\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import torch\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW4reS4J00J_"
      },
      "source": [
        "path_config  = '/content/drive/MyDrive/BERT/Config/config.json'\n",
        "path_model = '/content/drive/MyDrive/BERT/Config/pytorch_model.bin'\n",
        "path_bpe = '/content/drive/MyDrive/BERT/Config/bpe.codes'\n",
        "path_vocab = '/content/drive/MyDrive/BERT/Config/dict.txt'\n",
        "\n",
        "def get_model(path_model= None, path_config = None, path_bpe = None, path_vocab = None):\n",
        "  config = RobertaConfig.from_pretrained(\n",
        "      path_config, from_tf=False, num_labels = 3, output_hidden_states=False,\n",
        "  )\n",
        "  BERT_SA_NEW = RobertaForSequenceClassification.from_pretrained(\n",
        "      path_model,\n",
        "      config=config\n",
        "  )\n",
        "  BERT_SA_NEW.cuda()\n",
        "  BERT_SA_NEW.eval()\n",
        "\n",
        "\n",
        "  try:\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--bpe-codes', \n",
        "        default=path_bpe,\n",
        "        required=False,\n",
        "        type=str,\n",
        "        help='path to fastBPE BPE'\n",
        "    )\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    bpe = fastBPE(args)\n",
        "  except:\n",
        "    bpe = None\n",
        "    print(\"load bpe fail\")\n",
        "\n",
        "  try:\n",
        "    vocab = Dictionary()\n",
        "    vocab.add_from_file(path_vocab)\n",
        "  except:\n",
        "    vocab=None\n",
        "    print('load vocab fail')\n",
        "  return BERT_SA_NEW, bpe, vocab\n",
        "\n",
        "model, bpe, vocab = get_model(path_model, path_config, path_bpe, path_vocab)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yFrp8_6z52e"
      },
      "source": [
        "rdrsegmenter = VnCoreNLP(\"/content/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPntF43Hwph0"
      },
      "source": [
        "macp = pd.read_excel('/content/Macp.xlsx')\n",
        "macp = macp.dropna()\n",
        "tenct = macp['Tên Công ty'].tolist()\n",
        "for i in range(len(tenct)):\n",
        "  tenct[i] = str(tenct[i]).lower()\n",
        "tenct[:5]\n",
        "tenma = macp['Mã '].tolist()\n",
        "\n",
        "def del_test(text):\n",
        "  year = ['năm 2021', 'năm 2020', 'năm 2019', 'năm 2018', 'năm 2017', 'năm 2016', 'năm 2015', 'năm 2014', 'năm 2013', 'năm 2012', 'năm 2011', 'năm 2010', 'Năm 2021', 'Năm 2020', 'Năm 2019', 'Năm 2018', 'Năm 2017', 'Năm 2016', 'Năm 2015', 'Năm 2014', 'Năm 2013', 'Năm 2012', 'Năm 2011', 'Năm 2010', '2021', '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011', '2010']\n",
        "  month = ['tháng 1', 'tháng 2', 'tháng 3', 'tháng 4', 'tháng 5', 'tháng 6', 'tháng 7', 'tháng 8', 'tháng 9', 'tháng 10', 'tháng 11', 'tháng 12', 'Tháng 1', 'Tháng 2', 'Tháng 3', 'Tháng 4', 'Tháng 5', 'Tháng 6', 'Tháng 7', 'Tháng 8', 'Tháng 9', 'Tháng 10', 'Tháng 11', 'tháng 12']\n",
        "  quy = ['quý 1', 'quý 2', 'quý 3', 'quý 4', 'Quý 1', 'Quý 2', 'Quý 3', 'Quý 4']\n",
        "  text = text.replace('Covid-19', 'Covid')\n",
        "  word_segmented_text = rdrsegmenter.ner(text)[0]\n",
        "  for char, typ in word_segmentedtext:\n",
        "    if typ == 'B-ORG' or typ == 'I-ORG' or typ == 'B-PER' or typ == 'I-PER':\n",
        "      char = char.replace('', ' ')\n",
        "      text = text.replace(char, 'name')\n",
        "    if typ == \"B-LOC\" or typ == \"I-LOC\":\n",
        "      if char != 'VN':\n",
        "        char = char.replace('', ' ')\n",
        "        text = text.replace(char,'loc')\n",
        "    if typ == 'O':\n",
        "      if len(re.findall('\\d*.?,?\\d+%', char)) > 0:\n",
        "        text = text.replace(char, 'percent')\n",
        "      if len(re.findall('\\s?(?[A-Z]{3,4})?\\s?', char)) > 0 and char != 'USD':\n",
        "          text = text.replace(char, 'name')\n",
        "      if char in tenma:\n",
        "        text = text.replace(char, 'name')\n",
        "      char = char.replace('', ' ')\n",
        "      char_lower = char.lower()\n",
        "      if char_lower in tenct:\n",
        "        text = text.replace(char, 'name')\n",
        "  text = text.replace('\"', '')\n",
        "  text = text.replace('”', '')\n",
        "  text = text.replace('“', '')\n",
        "  text = text.replace('.', '')\n",
        "  text = text.replace(',', '')\n",
        "  text = text.replace('(', '')\n",
        "  text = text.replace(')', '')\n",
        "  text = text.replace(':', '')\n",
        "  text = text.replace('-', ' ')\n",
        "  text = re.sub('\\d{0,2}-?\\d{0,2}/\\d{1,4}', 'date', text)\n",
        "  for i in quy:\n",
        "    text = text.replace(i, 'date')\n",
        "  for i in year:\n",
        "    text = text.replace(i, 'date')\n",
        "  for i in month:\n",
        "    text = text.replace(i, 'date')\n",
        "  text = re.sub('\\d+ năm ', 'date ', text)\n",
        "  text = re.sub('\\d+ tháng ', 'date ', text)\n",
        "  text = re.sub(' -?\\d+\\w?', ' number', text)\n",
        "  text = text.split()\n",
        "  for i in range(len(text)):\n",
        "    if text[i].isdigit():\n",
        "      text[i] = 'number'\n",
        "  text = ' '.join(text)\n",
        "  text1 = text.split()\n",
        "  for i in range(len(text1)+1):\n",
        "    try:\n",
        "      if text1[i][0].isupper() and text1[i+1][0].isupper():\n",
        "        text = text.replace(text1[i], 'name')\n",
        "        text = text.replace(text1[i+1], 'name')\n",
        "    except:\n",
        "      pass\n",
        "  text = rdrsegmenter.tokenize(text)\n",
        "  text = ' '.join([' '.join(x) for x in text])\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ohMK4yDy9e8"
      },
      "source": [
        "def predict(model, bpe, sense, vocab):\n",
        "  subwords = '<s> ' + bpe.encode(sense) + ' </s>'\n",
        "  encoded_sent = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n",
        "  encoded_sent = pad_sequences([encoded_sent], maxlen=195, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "  mask = [int(token_id > 0) for token_id in encoded_sent[0]]\n",
        "\n",
        "\n",
        "  encoded_sent = torch.tensor(encoded_sent).cuda()\n",
        "  mask = torch.tensor(mask).cuda()\n",
        "  encoded_sent = torch.reshape(encoded_sent, (1, 195))\n",
        "  mask = torch.reshape(mask, (1, 195))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(encoded_sent, \n",
        "      token_type_ids=None, \n",
        "      attention_mask=mask)\n",
        "    logits = outputs[0]\n",
        "  return int(torch.argmax(logits))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f1tSTl1zBr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc742928-f4ef-4f88-dffa-342e752e6b38"
      },
      "source": [
        "sent = 'VIC tăng mạnh, giá trị cổ phiếu tỷ phú Phạm Nhật Vượng nắm giữ đạt xấp xỉ 220.000 tỷ đồng'\n",
        "predict(model, bpe, sent, vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}